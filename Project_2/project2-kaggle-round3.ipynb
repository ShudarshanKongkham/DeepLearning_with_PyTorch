{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(args1, *args):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # one of the best graphics library for python\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Define your custom log directory\n",
    "log_dir = 'TFlearning_logs'\n",
    "\n",
    "# Initialize the SummaryWriter with your log_dir\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12848), started 0:29:56 ago. (Use '!kill 12848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-496404ec00733b21\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-496404ec00733b21\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir=TFlearning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images and labels for the KenyanFood13 challenge.\n",
    "    This class reads a CSV file with columns 'id' and 'class', and loads images stored as '<id>.jpg'\n",
    "    in the specified image root directory. It also supports optional image resizing and transformations.\n",
    "    The train flag is used to split the full dataset into 80% training and 20% validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, image_root, train=True, image_shape=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initialization method.\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        csv_file (str): Path to the CSV file containing image ids and class labels.\n",
    "        \n",
    "        image_root (str): Directory where image files are stored. Images are expected to be named as '<id>.jpg'.\n",
    "        \n",
    "        train (bool): If True, returns 80% of the data for training; if False, returns 20% for validation.\n",
    "        \n",
    "        image_shape (int or tuple or list): [optional] If provided, image will be resized to the given shape.\n",
    "                                            If an integer is provided, images are resized to (image_shape, image_shape).\n",
    "                                            If a tuple/list is provided with one value, it is converted to square dimensions;\n",
    "                                            otherwise, the tuple is used as is.\n",
    "                                            \n",
    "        transform (callable): Transformation function to be applied on the PIL image.\n",
    "        \"\"\"\n",
    "        # Read the CSV file containing the image ids and their class labels\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_root = image_root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Process image_shape parameter\n",
    "        if image_shape is not None:\n",
    "            if isinstance(image_shape, int):\n",
    "                self.image_shape = (image_shape, image_shape)\n",
    "            elif isinstance(image_shape, (tuple, list)):\n",
    "                assert len(image_shape) in [1, 2], 'Invalid image_shape tuple/list size'\n",
    "                self.image_shape = (image_shape[0], image_shape[0]) if len(image_shape) == 1 else image_shape\n",
    "            else:\n",
    "                raise NotImplementedError(\"image_shape must be an int, tuple, or list\")\n",
    "        else:\n",
    "            self.image_shape = None\n",
    "        \n",
    "        # Create a mapping from class name to integer label\n",
    "        self.classes = sorted(self.data['class'].unique())\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Build the list of samples (each sample is a tuple of image path and numeric label)\n",
    "        full_samples = []\n",
    "        for _, row in self.data.iterrows():\n",
    "            img_id = row['id']\n",
    "            label_str = row['class']\n",
    "            label = self.class_to_idx[label_str]\n",
    "            img_path = os.path.join(self.image_root, f\"{img_id}.jpg\")\n",
    "            full_samples.append((img_path, label))\n",
    "        \n",
    "        # Shuffle the full sample list with a fixed seed for reproducibility and split into train/validation.\n",
    "        random.seed(42)\n",
    "        random.shuffle(full_samples)\n",
    "        n_train = int(0.8 * len(full_samples))\n",
    "        if self.train:\n",
    "            self.samples = full_samples[:n_train]\n",
    "        else:\n",
    "            self.samples = full_samples[n_train:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For the given index, returns the processed image and its label.\n",
    "        \n",
    "        Parameters:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            image: Processed image.\n",
    "            target (int): Numeric label for the image.\n",
    "        \"\"\"\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize image if image_shape is specified\n",
    "        if self.image_shape is not None:\n",
    "            image = TF.resize(image, self.image_shape)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def class_name(self, label):\n",
    "        \"\"\"\n",
    "        Gets the class name corresponding to a numeric label.\n",
    "        \n",
    "        Parameters:\n",
    "            label (int): Numeric label.\n",
    "        \n",
    "        Returns:\n",
    "            str: Class name.\n",
    "        \"\"\"\n",
    "        return self.classes[label]\n",
    "\n",
    "\n",
    "def get_data(batch_size, data_root, tb_writer, num_workers=4, data_augmentation=True, csv_file=None, image_shape=None):\n",
    "    if csv_file is None:\n",
    "        csv_file = os.path.join(data_root, 'train.csv')\n",
    "    image_root = os.path.join(data_root, 'images/images')\n",
    "    \n",
    "    mean, std = get_mean_std()\n",
    "    common_transforms = image_common_transforms(mean, std)\n",
    "    \n",
    "    if data_augmentation:\n",
    "        train_transforms = data_augmentation_preprocess(mean, std)\n",
    "    else:\n",
    "        train_transforms = common_transforms\n",
    "    \n",
    "    # Use the modified data_loader with train flag instead of shuffle\n",
    "    train_loader = data_loader(csv_file,\n",
    "                               image_root,\n",
    "                               train_transforms,\n",
    "                               batch_size=batch_size,\n",
    "                               train=True,\n",
    "                               image_shape=image_shape,\n",
    "                               num_workers=num_workers)\n",
    "    \n",
    "    test_loader = data_loader(csv_file,\n",
    "                              image_root,\n",
    "                              common_transforms,\n",
    "                              batch_size=batch_size,\n",
    "                              train=False,\n",
    "                              image_shape=image_shape,\n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 21  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  \n",
    "    epochs_count: int = 20 \n",
    "    init_learning_rate: float = 0.001  # initial learning rate for lr scheduler\n",
    "    decay_rate: float = 0.1  \n",
    "    log_interval: int = 500  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = './opencv-pytorch-project-2-classification-round-3/'\n",
    "    csv_file: str = './opencv-pytorch-project-2-classification-round-3/train.csv'\n",
    "    num_workers: int = 10  \n",
    "    device: str = 'cuda'  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bhaji',\n",
       " 'chapati',\n",
       " 'githeri',\n",
       " 'kachumbari',\n",
       " 'kukuchoma',\n",
       " 'mandazi',\n",
       " 'masalachips',\n",
       " 'matoke',\n",
       " 'mukimo',\n",
       " 'nyamachoma',\n",
       " 'pilau',\n",
       " 'sukumawiki',\n",
       " 'ugali']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# food_classes = ['Ugali', 'Chapati', 'Mukimo', 'Matoke', 'Kachumbari', 'Sukuma Wiki', 'Nyama Choma', 'Githeri', 'Mandazi', 'Pilau', 'Kebab', 'Samaki', 'Wali']\n",
    "dataset = KenyanFood13Dataset(TrainingConfiguration.csv_file, TrainingConfiguration.data_root, train=False, image_shape=224)\n",
    "food_classes = dataset.classes\n",
    "food_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input, max_prob=True):\n",
    "    \"\"\"\n",
    "    get prediction for batch inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # send model to cpu/cuda according to your system configuration\n",
    "    model.to(device)\n",
    "    \n",
    "    # it is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_input.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # get probability score using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "    \n",
    "    if max_prob:\n",
    "        # get the max probability\n",
    "        pred_prob = prob.data.max(dim=1)[0]\n",
    "    else:\n",
    "        pred_prob = prob.data\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_target_and_prob(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    get targets and prediction probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_prob = []\n",
    "    targets = []\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        _, prob = prediction(model, device, data, max_prob=False)\n",
    "        \n",
    "        pred_prob.append(prob)\n",
    "        \n",
    "        target = target.numpy()\n",
    "        targets.append(target)\n",
    "        \n",
    "    targets = np.concatenate(targets)\n",
    "    targets = targets.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob, axis=0)\n",
    "    \n",
    "    return targets, pred_prob\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (it is mandatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n",
    "            \n",
    "            total_batch = epoch_idx * len(train_loader.dataset)/train_config.batch_size + batch_idx\n",
    "            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n",
    "            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=False):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        tb_writer=tb_writer,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "    \n",
    "    \n",
    "    # add network graph with inputs info\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(training_configuration.device)\n",
    "    add_network_graph_tensorboard(model, images, tb_writer)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epoch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    add_wrong_prediction_to_tensorboard(model, test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer, 0, max_images=300)\n",
    "    \n",
    "    \n",
    "    # training time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Traing\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "        \n",
    "        # add scalar (loss/accuracy) to tensorboard\n",
    "        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n",
    "        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        # add time metadata to tensorboard\n",
    "        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n",
    "        tb_writer.add_scalar('Time/eta', eta, epoch)\n",
    "        \n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            # add scalar (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n",
    "            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n",
    "            \n",
    "            # add scalars (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n",
    "                                           'validation': current_loss}, epoch)\n",
    "            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n",
    "                                               'validation': current_accuracy}, epoch)\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "            # add wrong predicted image to tensorboard\n",
    "            add_wrong_prediction_to_tensorboard(model, test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer, epoch, max_images=300)\n",
    "        \n",
    "        # scheduler step/ update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # adding model weights to tensorboard as histogram\n",
    "        add_model_weights_as_histogram(model, tb_writer, epoch)\n",
    "        \n",
    "        # add pr curves to tensor board\n",
    "        add_pr_curves_to_tensorboard(model, test_loader, \n",
    "                                     training_configuration.device, \n",
    "                                     tb_writer, epoch, num_classes=3)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pretrained_resnet18(transfer_learning=True, num_class=3):\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    \n",
    "    if transfer_learning:\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    last_layer_in = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(last_layer_in, num_class)\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def image_preprocess_transforms():\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess\n",
    "\n",
    "def image_common_transforms(mean, std):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms\n",
    "\n",
    "def data_augmentation_preprocess(mean, std):\n",
    "    \n",
    "    initail_transoform = transforms.RandomChoice([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90)\n",
    "        ])\n",
    "    \n",
    "    common_transforms = image_common_transforms(mean, std)\n",
    "                \n",
    "    aug_transforms = transforms.Compose([\n",
    "        initail_transoform,\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        common_transforms\n",
    "        ])\n",
    "    \n",
    "    return aug_transforms\n",
    "    \n",
    "def data_loader(csv_file, image_root, transform, batch_size=16, train=False, image_shape=None, num_workers=2):\n",
    "    \"\"\"\n",
    "    New data loader using KenyanFood13Dataset.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV with image IDs and labels.\n",
    "        image_root (str): Directory containing the images.\n",
    "        transform (callable): Transformations to apply.\n",
    "        batch_size (int): Batch size.\n",
    "        train (bool): Determines if dataset is for training (affects shuffling).\n",
    "        image_shape (int, tuple, or list): Optional image resize dimensions.\n",
    "        num_workers (int): Number of worker threads.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: Data loader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = KenyanFood13Dataset(csv_file, image_root, train=train, image_shape=image_shape, transform=transform)\n",
    "    loader = DataLoader(dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=train)\n",
    "    return loader\n",
    "\n",
    "def get_mean_std():\n",
    "    \n",
    "    mean = [0.485, 0.456, 0.406] \n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch, num_classes=3):\n",
    "    \"\"\"\n",
    "    Add precession and recall curve to tensorboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets, pred_prob = get_target_and_prob(model, dataloader, device)\n",
    "    \n",
    "    for cls_idx in range(num_classes):\n",
    "        binary_target = targets == cls_idx\n",
    "        true_prediction_prob = pred_prob[:, cls_idx]\n",
    "        \n",
    "        tb_writer.add_pr_curve(food_classes[cls_idx], \n",
    "                               binary_target, \n",
    "                               true_prediction_prob, \n",
    "                               global_step=epoch)\n",
    "        \n",
    "    return\n",
    "\n",
    "def add_wrong_prediction_to_tensorboard(model, dataloader, device, tb_writer, \n",
    "                                        epoch, tag='Wrong_Predections', max_images='all'):\n",
    "    \"\"\"\n",
    "    Add wrong predicted images to tensorboard.\n",
    "    \"\"\"\n",
    "    #number of images in one row\n",
    "    num_images_per_row = 8\n",
    "    im_scale = 3\n",
    "    \n",
    "    plot_images = []\n",
    "    wrong_labels = []\n",
    "    pred_prob = []\n",
    "    right_label = []\n",
    "    \n",
    "    mean, std = get_mean_std()\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        images = data.numpy()\n",
    "        pred, prob = prediction(model, device, data)\n",
    "        target = target.numpy()\n",
    "        indices = pred.astype(int) != target.astype(int)\n",
    "        \n",
    "        plot_images.append(images[indices])\n",
    "        wrong_labels.append(pred[indices])\n",
    "        pred_prob.append(prob[indices])\n",
    "        right_label.append(target[indices])\n",
    "        \n",
    "    plot_images = np.concatenate(plot_images, axis=0).squeeze()\n",
    "    plot_images = (np.moveaxis(plot_images, 1, -1) * std) + mean\n",
    "    wrong_labels = np.concatenate(wrong_labels)\n",
    "    wrong_labels = wrong_labels.astype(int)\n",
    "    right_label = np.concatenate(right_label)\n",
    "    right_label = right_label.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob)\n",
    "    \n",
    "    \n",
    "    if max_images == 'all':\n",
    "        num_images = len(images)\n",
    "    else:\n",
    "        num_images = min(len(plot_images), max_images)\n",
    "        \n",
    "    fig_width = num_images_per_row * im_scale\n",
    "    \n",
    "    if num_images % num_images_per_row == 0:\n",
    "        num_row = num_images/num_images_per_row\n",
    "    else:\n",
    "        num_row = int(num_images/num_images_per_row) + 1\n",
    "        \n",
    "    fig_height = num_row * im_scale\n",
    "        \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams[\"figure.figsize\"] = (fig_width, fig_height)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_row, num_images_per_row, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow((plot_images[i]*255).astype(np.uint8))\n",
    "        plt.gca().set_title('{0}({1:.2}), {2}'.format(animal_classes[wrong_labels[i]], \n",
    "                                                          pred_prob[i], \n",
    "                                                          animal_classes[right_label[i]]))\n",
    "        \n",
    "    tb_writer.add_figure(tag, fig, global_step=epoch)\n",
    "    \n",
    "    return\n",
    "\n",
    "    \n",
    "def add_model_weights_as_histogram(model, tb_writer, epoch):\n",
    "    for name, param in model.named_parameters():\n",
    "        tb_writer.add_histogram(name.replace('.', '/'), param.data.cpu().abs(), epoch)\n",
    "    return\n",
    "\n",
    "\n",
    "def add_network_graph_tensorboard(model, inputs, tb_writer):\n",
    "    tb_writer.add_graph(model, inputs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=False):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        tb_writer=tb_writer,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "    \n",
    "    \n",
    "    # add network graph with inputs info\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(training_configuration.device)\n",
    "    add_network_graph_tensorboard(model, images, tb_writer)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epoch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    add_wrong_prediction_to_tensorboard(model, test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer, 0, max_images=300)\n",
    "    \n",
    "    \n",
    "    # training time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Traing\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "        \n",
    "        # add scalar (loss/accuracy) to tensorboard\n",
    "        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n",
    "        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        # add time metadata to tensorboard\n",
    "        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n",
    "        tb_writer.add_scalar('Time/eta', eta, epoch)\n",
    "        \n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            # add scalar (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n",
    "            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n",
    "            \n",
    "            # add scalars (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n",
    "                                           'validation': current_loss}, epoch)\n",
    "            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n",
    "                                               'validation': current_accuracy}, epoch)\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "            # add wrong predicted image to tensorboard\n",
    "            add_wrong_prediction_to_tensorboard(model, test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer, epoch, max_images=300)\n",
    "        \n",
    "        # scheduler step/ update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # adding model weights to tensorboard as histogram\n",
    "        add_model_weights_as_histogram(model, tb_writer, epoch)\n",
    "        \n",
    "        # add pr curves to tensor board\n",
    "        add_pr_curves_to_tensorboard(model, test_loader, \n",
    "                                     training_configuration.device, \n",
    "                                     tb_writer, epoch, num_classes=3)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model):\n",
    "    train_config = TrainingConfiguration()\n",
    "\n",
    "    init_learning_rate = train_config.init_learning_rate\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr = init_learning_rate,\n",
    "        momentum = 0.9\n",
    "    )\n",
    "\n",
    "    decay_rate = train_config.decay_rate\n",
    "\n",
    "    lmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = pretrained_resnet18(transfer_learning=True)\n",
    "print(model)\n",
    "# get optimizer and scheduler\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(model)\n",
    "\n",
    "# Tensorboard summary writer\n",
    "transfer_learning_sw = SummaryWriter('log_resnet18/transfer_learning')   \n",
    "\n",
    "# train and validate\n",
    "model, train_loss_exp2, train_acc_exp2, val_loss_exp2, val_acc_exp2 = main(model, \n",
    "                                                                           optimizer,\n",
    "                                                                           transfer_learning_sw,\n",
    "                                                                           scheduler,\n",
    "                                                                           data_augmentation=True)\n",
    "transfer_learning_sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "Note: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n",
    "\n",
    "You are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10665762,
     "sourceId": 90936,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
