{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Table of contents</font>\n",
    "\n",
    "- [LeNet5 Architecture](#lenet)\n",
    "- [Display the Network](#display)\n",
    "- [Get the Fashion-MNIST Data](#get-data)\n",
    "- [System Configuration](#sys-config)\n",
    "- [Training Configuration](#train-config)\n",
    "- [System Setup](#sys-setup)\n",
    "- [Training](#training)\n",
    "- [Validation](#validation)\n",
    "- [Main function](#main)\n",
    "- [Plot Loss](#plot-loss)\n",
    "- [Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
    "\n",
    "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "\n",
    "We want to classify images in this dataset, using the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
    "\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
    "\n",
    "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            indx_target = target.clone()\n",
    "            data = data.to(train_config.device)\n",
    "\n",
    "            target = target.to(train_config.device)\n",
    "\n",
    "            output = model(data)\n",
    "            # add loss for each mini batch\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "            # get probability score using softmax\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "            # get the index of the max probability\n",
    "            pred = prob.data.max(dim=1)[1] \n",
    "\n",
    "            # add correct prediction count\n",
    "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # average over number of mini-batches\n",
    "        test_loss = test_loss / len(test_loader)  \n",
    "\n",
    "        # average over number of dataset\n",
    "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "            )\n",
    "        )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
    "\n",
    "\n",
    "Here, we use the configuration parameters defined above and start  training. \n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:46<00:00, 573139.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 107805.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:08<00:00, 526442.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Train Epoch: 0 [3200/60000] Loss: 2.283236 Acc: 0.1875\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.256051 Acc: 0.2500\n",
      "Train Epoch: 0 [9600/60000] Loss: 2.017986 Acc: 0.3750\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.162067 Acc: 0.4375\n",
      "Train Epoch: 0 [16000/60000] Loss: 1.065939 Acc: 0.5938\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.837235 Acc: 0.6875\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.741086 Acc: 0.7188\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.887732 Acc: 0.6250\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.681480 Acc: 0.7188\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.678257 Acc: 0.7188\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.819761 Acc: 0.6250\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.636804 Acc: 0.7500\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.601070 Acc: 0.7500\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.989290 Acc: 0.6250\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.645001 Acc: 0.7500\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.576932 Acc: 0.7188\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.740906 Acc: 0.7188\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.717522 Acc: 0.7188\n",
      "Elapsed 17.85s, 17.85 s/epoch, 0.01 s/batch, ets 339.08s\n",
      "\n",
      "Test set: Average loss: 0.6454, Accuracy: 7462/10000 (75%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.693895 Acc: 0.6875\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.533333 Acc: 0.7812\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.559411 Acc: 0.6875\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.762960 Acc: 0.8125\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.514729 Acc: 0.9062\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.712124 Acc: 0.6562\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.400903 Acc: 0.8750\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.495331 Acc: 0.8438\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.438394 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.348349 Acc: 0.8750\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.363213 Acc: 0.9062\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.560796 Acc: 0.8438\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.566799 Acc: 0.7812\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.589122 Acc: 0.8125\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.602213 Acc: 0.7500\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.432475 Acc: 0.8125\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.764057 Acc: 0.6875\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.605274 Acc: 0.7500\n",
      "Elapsed 46.96s, 23.48 s/epoch, 0.01 s/batch, ets 422.63s\n",
      "\n",
      "Test set: Average loss: 0.5509, Accuracy: 7946/10000 (79%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.613414 Acc: 0.7812\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.513840 Acc: 0.8438\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.254093 Acc: 0.9062\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.500271 Acc: 0.8438\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.270828 Acc: 0.9375\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.959751 Acc: 0.7188\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.451374 Acc: 0.8438\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.403139 Acc: 0.8438\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.552312 Acc: 0.8125\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.139168 Acc: 1.0000\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.719959 Acc: 0.6875\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.394937 Acc: 0.8438\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.626213 Acc: 0.8438\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.639396 Acc: 0.8125\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.254216 Acc: 0.9375\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.323733 Acc: 0.9062\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.308442 Acc: 0.9062\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.449387 Acc: 0.8438\n",
      "Elapsed 78.04s, 26.01 s/epoch, 0.01 s/batch, ets 442.21s\n",
      "\n",
      "Test set: Average loss: 0.4742, Accuracy: 8262/10000 (83%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.379578 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.288327 Acc: 0.9375\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.460560 Acc: 0.8750\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.681731 Acc: 0.7812\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.411391 Acc: 0.8438\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.493313 Acc: 0.7500\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.745966 Acc: 0.7812\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.330007 Acc: 0.9375\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.501146 Acc: 0.8438\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.254879 Acc: 0.9062\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.330803 Acc: 0.9062\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.214396 Acc: 0.9688\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.401374 Acc: 0.8125\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.266740 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.830002 Acc: 0.6250\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.944688 Acc: 0.7500\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.733480 Acc: 0.6875\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.250778 Acc: 0.9375\n",
      "Elapsed 107.59s, 26.90 s/epoch, 0.01 s/batch, ets 430.36s\n",
      "\n",
      "Test set: Average loss: 0.4466, Accuracy: 8345/10000 (83%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.325665 Acc: 0.8750\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.340688 Acc: 0.8750\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.556702 Acc: 0.7812\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.354805 Acc: 0.8125\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.162673 Acc: 0.9688\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.518485 Acc: 0.8125\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.324192 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.312972 Acc: 0.8750\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.445243 Acc: 0.8438\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.346993 Acc: 0.9062\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.391352 Acc: 0.8125\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.305925 Acc: 0.8750\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.313815 Acc: 0.9062\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.238276 Acc: 0.9062\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.348931 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.565450 Acc: 0.8125\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.276304 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.483404 Acc: 0.7812\n",
      "Elapsed 127.17s, 25.43 s/epoch, 0.01 s/batch, ets 381.50s\n",
      "\n",
      "Test set: Average loss: 0.4176, Accuracy: 8474/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.592026 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.357936 Acc: 0.9062\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.301574 Acc: 0.8750\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.430536 Acc: 0.8125\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.433063 Acc: 0.8125\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.371806 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.417280 Acc: 0.8125\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.151067 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.381712 Acc: 0.8438\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.325064 Acc: 0.8750\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.245288 Acc: 0.9062\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.368934 Acc: 0.8438\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.299188 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.235754 Acc: 0.8750\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.591660 Acc: 0.7500\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.191122 Acc: 0.9375\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.251497 Acc: 0.9688\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.302750 Acc: 0.9375\n",
      "Elapsed 146.22s, 24.37 s/epoch, 0.01 s/batch, ets 341.19s\n",
      "\n",
      "Test set: Average loss: 0.4104, Accuracy: 8502/10000 (85%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.130671 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.328430 Acc: 0.8125\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.272669 Acc: 0.9062\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.236539 Acc: 0.9062\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.227304 Acc: 0.9375\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.125210 Acc: 0.9375\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.276567 Acc: 0.9062\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.386837 Acc: 0.8125\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.269340 Acc: 0.8750\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.272758 Acc: 0.8750\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.277219 Acc: 0.8750\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.493884 Acc: 0.7812\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.350744 Acc: 0.8750\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.605598 Acc: 0.8125\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.302091 Acc: 0.8438\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.304104 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.300889 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.459384 Acc: 0.8125\n",
      "Elapsed 165.47s, 23.64 s/epoch, 0.01 s/batch, ets 307.30s\n",
      "\n",
      "Test set: Average loss: 0.4035, Accuracy: 8531/10000 (85%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.348101 Acc: 0.9062\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.613437 Acc: 0.7500\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.418893 Acc: 0.8438\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.230009 Acc: 0.8750\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.387849 Acc: 0.8750\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.603835 Acc: 0.8125\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.238068 Acc: 0.8750\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.448004 Acc: 0.8750\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.386957 Acc: 0.8438\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.386640 Acc: 0.8750\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.407426 Acc: 0.8125\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.470850 Acc: 0.8125\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.342008 Acc: 0.8438\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.216137 Acc: 0.8750\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.414378 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.272035 Acc: 0.9062\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.244692 Acc: 0.9375\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.214416 Acc: 0.8750\n",
      "Elapsed 184.63s, 23.08 s/epoch, 0.01 s/batch, ets 276.95s\n",
      "\n",
      "Test set: Average loss: 0.3537, Accuracy: 8693/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.572391 Acc: 0.7812\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.411746 Acc: 0.7812\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.348697 Acc: 0.8750\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.255933 Acc: 0.8750\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.209774 Acc: 0.9375\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.195541 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.358452 Acc: 0.8438\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.307293 Acc: 0.8750\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.219865 Acc: 0.9375\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.114275 Acc: 1.0000\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.237894 Acc: 0.8750\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.455893 Acc: 0.8438\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.244657 Acc: 0.9062\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.560047 Acc: 0.7500\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.211123 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.284470 Acc: 0.8438\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.111313 Acc: 0.9375\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.150554 Acc: 0.9688\n",
      "Elapsed 203.55s, 22.62 s/epoch, 0.01 s/batch, ets 248.79s\n",
      "\n",
      "Test set: Average loss: 0.3469, Accuracy: 8710/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.263942 Acc: 0.9062\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.208821 Acc: 0.9375\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.248209 Acc: 0.9375\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.175301 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.230907 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.270284 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.349239 Acc: 0.8750\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.342617 Acc: 0.8750\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.299562 Acc: 0.8438\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.269105 Acc: 0.9062\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.405180 Acc: 0.8750\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.291392 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.371128 Acc: 0.8750\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.585496 Acc: 0.8125\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.382309 Acc: 0.8750\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.455334 Acc: 0.9062\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.263843 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.214834 Acc: 0.9062\n",
      "Elapsed 222.42s, 22.24 s/epoch, 0.01 s/batch, ets 222.42s\n",
      "\n",
      "Test set: Average loss: 0.3434, Accuracy: 8717/10000 (87%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.336721 Acc: 0.9062\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.320108 Acc: 0.8750\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.183826 Acc: 0.9062\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.378856 Acc: 0.8438\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.219651 Acc: 0.9375\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.496582 Acc: 0.9375\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.362953 Acc: 0.8750\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.295524 Acc: 0.9375\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.512155 Acc: 0.8438\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.394547 Acc: 0.8438\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.179951 Acc: 0.9062\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.137798 Acc: 0.9688\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.126002 Acc: 0.9688\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.327548 Acc: 0.8750\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.207494 Acc: 0.9062\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.228760 Acc: 0.9062\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.243813 Acc: 0.8750\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.520402 Acc: 0.8125\n",
      "Elapsed 242.74s, 22.07 s/epoch, 0.01 s/batch, ets 198.61s\n",
      "\n",
      "Test set: Average loss: 0.3322, Accuracy: 8784/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.197383 Acc: 0.9375\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.429836 Acc: 0.7812\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.164830 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.456198 Acc: 0.8438\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.347808 Acc: 0.8750\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.293704 Acc: 0.9062\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.169733 Acc: 0.9375\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.092689 Acc: 0.9688\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.582983 Acc: 0.8125\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.142644 Acc: 0.9688\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.195825 Acc: 0.9375\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.339022 Acc: 0.9062\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.180800 Acc: 0.9375\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.254683 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.249996 Acc: 0.8750\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.187552 Acc: 0.9062\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.136601 Acc: 1.0000\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.359765 Acc: 0.8438\n",
      "Elapsed 262.35s, 21.86 s/epoch, 0.01 s/batch, ets 174.90s\n",
      "\n",
      "Test set: Average loss: 0.3274, Accuracy: 8813/10000 (88%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.323823 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.333552 Acc: 0.8750\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.290343 Acc: 0.9062\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.257525 Acc: 0.9375\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.151373 Acc: 0.9375\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.503857 Acc: 0.8125\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.265795 Acc: 0.9375\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.210090 Acc: 0.9062\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.438540 Acc: 0.8438\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.206401 Acc: 0.9375\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.154050 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.421719 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.193686 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.130349 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.077079 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.303072 Acc: 0.9062\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.584805 Acc: 0.7812\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.614488 Acc: 0.8125\n",
      "Elapsed 281.62s, 21.66 s/epoch, 0.01 s/batch, ets 151.64s\n",
      "\n",
      "Test set: Average loss: 0.3138, Accuracy: 8865/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.321310 Acc: 0.8750\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.267176 Acc: 0.8750\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.223453 Acc: 0.9062\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.266602 Acc: 0.9375\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.147831 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.305213 Acc: 0.8438\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.177827 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.448003 Acc: 0.8438\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.166005 Acc: 0.9375\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.198046 Acc: 0.9375\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.593620 Acc: 0.7812\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.202392 Acc: 0.9375\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.284864 Acc: 0.8438\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.053189 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.144989 Acc: 0.9688\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.372333 Acc: 0.9062\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.434454 Acc: 0.8750\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.183509 Acc: 0.9062\n",
      "Elapsed 302.02s, 21.57 s/epoch, 0.01 s/batch, ets 129.44s\n",
      "\n",
      "Test set: Average loss: 0.3122, Accuracy: 8894/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.214467 Acc: 0.8750\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.435067 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.309513 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.131985 Acc: 1.0000\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.287462 Acc: 0.8750\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.172681 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.579645 Acc: 0.8750\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.191058 Acc: 0.9688\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.638486 Acc: 0.7812\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.152690 Acc: 0.9375\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.321565 Acc: 0.9375\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.316073 Acc: 0.9062\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.158192 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.331538 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.193959 Acc: 0.9062\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.319505 Acc: 0.8750\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.286876 Acc: 0.8750\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.306370 Acc: 0.8438\n",
      "Elapsed 321.69s, 21.45 s/epoch, 0.01 s/batch, ets 107.23s\n",
      "\n",
      "Test set: Average loss: 0.3169, Accuracy: 8844/10000 (88%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.461971 Acc: 0.8125\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.181148 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.094877 Acc: 0.9688\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.197642 Acc: 0.9375\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.201034 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.302315 Acc: 0.8438\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.397532 Acc: 0.8750\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.262885 Acc: 0.8438\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.444255 Acc: 0.8125\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.260678 Acc: 0.8438\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.276348 Acc: 0.8750\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.235959 Acc: 0.9375\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.442316 Acc: 0.8750\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.449037 Acc: 0.8750\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.537373 Acc: 0.8750\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.203719 Acc: 0.9375\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.230772 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.258720 Acc: 0.8438\n",
      "Elapsed 341.36s, 21.33 s/epoch, 0.01 s/batch, ets 85.34s\n",
      "\n",
      "Test set: Average loss: 0.3185, Accuracy: 8861/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.150291 Acc: 0.9688\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.188694 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.253967 Acc: 0.9375\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.138395 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.264589 Acc: 0.9062\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.216208 Acc: 0.9375\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.169231 Acc: 0.9062\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.499632 Acc: 0.7812\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.409866 Acc: 0.8750\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.258564 Acc: 0.8750\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.201632 Acc: 0.9688\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.346223 Acc: 0.8750\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.244690 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.256026 Acc: 0.9062\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.221104 Acc: 0.8750\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.048691 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.221336 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.162897 Acc: 0.9375\n",
      "Elapsed 360.71s, 21.22 s/epoch, 0.01 s/batch, ets 63.66s\n",
      "\n",
      "Test set: Average loss: 0.3065, Accuracy: 8908/10000 (89%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.283924 Acc: 0.8438\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.160277 Acc: 0.9688\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.406686 Acc: 0.8750\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.163671 Acc: 0.9062\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.197557 Acc: 0.9375\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.134087 Acc: 0.9688\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.229042 Acc: 0.9062\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.313252 Acc: 0.8438\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.327139 Acc: 0.9375\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.211576 Acc: 0.9688\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.161250 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.177121 Acc: 0.9375\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.216190 Acc: 0.9375\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.380914 Acc: 0.8750\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.356284 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.238417 Acc: 0.9062\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.280253 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.318357 Acc: 0.9062\n",
      "Elapsed 379.89s, 21.11 s/epoch, 0.01 s/batch, ets 42.21s\n",
      "\n",
      "Test set: Average loss: 0.3166, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.364871 Acc: 0.8438\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.159421 Acc: 0.9062\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.292321 Acc: 0.9062\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.096128 Acc: 0.9688\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.130338 Acc: 0.9688\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.139365 Acc: 0.9688\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.129956 Acc: 0.9688\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.455143 Acc: 0.8125\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.106082 Acc: 1.0000\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.254593 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.259350 Acc: 0.9375\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.184706 Acc: 0.9375\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.252067 Acc: 0.9062\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.245437 Acc: 0.8438\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.130854 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.302830 Acc: 0.8750\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.231458 Acc: 0.9062\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.272890 Acc: 0.9062\n",
      "Elapsed 399.53s, 21.03 s/epoch, 0.01 s/batch, ets 21.03s\n",
      "\n",
      "Test set: Average loss: 0.3150, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.334310 Acc: 0.8750\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.171943 Acc: 0.9375\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.212360 Acc: 0.9688\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.216907 Acc: 0.9062\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.246295 Acc: 0.9062\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.279490 Acc: 0.9062\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.059624 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.267558 Acc: 0.8750\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.143631 Acc: 0.9375\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.270542 Acc: 0.8750\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.223026 Acc: 0.9375\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.238307 Acc: 0.9062\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.261891 Acc: 0.9062\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.400140 Acc: 0.9062\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.092011 Acc: 0.9688\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.234496 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.306784 Acc: 0.8438\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.337830 Acc: 0.8125\n",
      "Elapsed 418.98s, 20.95 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3117, Accuracy: 8853/10000 (89%)\n",
      "\n",
      "Total time: 425.91, Best Loss: 0.306\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.801559 Acc: 0.5625\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.047881 Acc: 0.6562\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.891813 Acc: 0.7500\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.756786 Acc: 0.6562\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.744260 Acc: 0.8125\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.626530 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.477027 Acc: 0.8438\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.622751 Acc: 0.7500\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.565109 Acc: 0.8125\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.513895 Acc: 0.8438\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.684682 Acc: 0.6875\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.471840 Acc: 0.8125\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.510493 Acc: 0.8438\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.567848 Acc: 0.8125\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.529899 Acc: 0.7812\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.400740 Acc: 0.8438\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.447175 Acc: 0.8438\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.509230 Acc: 0.8750\n",
      "Elapsed 13.12s, 13.12 s/epoch, 0.01 s/batch, ets 249.24s\n",
      "\n",
      "Test set: Average loss: 0.4647, Accuracy: 8306/10000 (83%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.602158 Acc: 0.7500\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.668300 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.520420 Acc: 0.7812\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.691175 Acc: 0.7500\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.508233 Acc: 0.8750\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.753843 Acc: 0.7500\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.240368 Acc: 0.9062\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.357451 Acc: 0.8438\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.361066 Acc: 0.8438\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.299686 Acc: 0.9062\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.255262 Acc: 0.9375\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.499144 Acc: 0.7500\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.272556 Acc: 0.9375\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.413647 Acc: 0.7812\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.444346 Acc: 0.8125\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.280321 Acc: 0.9062\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.612918 Acc: 0.7188\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.506598 Acc: 0.8125\n",
      "Elapsed 33.05s, 16.52 s/epoch, 0.01 s/batch, ets 297.41s\n",
      "\n",
      "Test set: Average loss: 0.3940, Accuracy: 8600/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.436742 Acc: 0.8750\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.342818 Acc: 0.8750\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.126418 Acc: 1.0000\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.406859 Acc: 0.9062\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.248733 Acc: 0.8750\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.593659 Acc: 0.7812\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.400194 Acc: 0.8750\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.249220 Acc: 0.9375\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.383378 Acc: 0.9062\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.104008 Acc: 0.9688\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.438184 Acc: 0.8438\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.281389 Acc: 0.8438\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.337449 Acc: 0.9688\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.271032 Acc: 0.8750\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.147163 Acc: 0.9688\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.218888 Acc: 0.9688\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.210500 Acc: 0.9062\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.293094 Acc: 0.9688\n",
      "Elapsed 53.17s, 17.72 s/epoch, 0.01 s/batch, ets 301.27s\n",
      "\n",
      "Test set: Average loss: 0.3894, Accuracy: 8587/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.277405 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.283265 Acc: 0.8750\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.556879 Acc: 0.7812\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.551800 Acc: 0.8438\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.275893 Acc: 0.9062\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.313820 Acc: 0.8438\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.474998 Acc: 0.8750\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.393344 Acc: 0.8438\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.294337 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.173579 Acc: 0.9062\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.418775 Acc: 0.8438\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.148413 Acc: 0.9375\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.413624 Acc: 0.8438\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.178157 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.758749 Acc: 0.7188\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.790817 Acc: 0.8125\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.375721 Acc: 0.8438\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.193472 Acc: 0.9375\n",
      "Elapsed 73.06s, 18.27 s/epoch, 0.01 s/batch, ets 292.25s\n",
      "\n",
      "Test set: Average loss: 0.3423, Accuracy: 8782/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.266814 Acc: 0.9062\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.317091 Acc: 0.9062\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.283771 Acc: 0.8750\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.220813 Acc: 0.9688\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.148871 Acc: 0.9688\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.390414 Acc: 0.8750\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.304623 Acc: 0.9375\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.243109 Acc: 0.9375\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.318249 Acc: 0.8438\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.286661 Acc: 0.9062\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.337432 Acc: 0.8438\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.344536 Acc: 0.8438\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.249229 Acc: 0.9062\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.141402 Acc: 0.9688\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.214025 Acc: 0.9375\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.389239 Acc: 0.7812\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.205507 Acc: 0.9375\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.256514 Acc: 0.9062\n",
      "Elapsed 93.08s, 18.62 s/epoch, 0.01 s/batch, ets 279.25s\n",
      "\n",
      "Test set: Average loss: 0.3273, Accuracy: 8822/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.322282 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.278814 Acc: 0.9375\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.270770 Acc: 0.9062\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.395029 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.367423 Acc: 0.8750\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.355673 Acc: 0.8750\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.219597 Acc: 0.8750\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.101244 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.288059 Acc: 0.8750\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.217403 Acc: 0.9062\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.088417 Acc: 0.9688\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.339883 Acc: 0.8750\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.154828 Acc: 0.9062\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.265285 Acc: 0.8750\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.595015 Acc: 0.7188\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.144653 Acc: 0.9688\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.159183 Acc: 0.9375\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.165615 Acc: 1.0000\n",
      "Elapsed 113.15s, 18.86 s/epoch, 0.01 s/batch, ets 264.02s\n",
      "\n",
      "Test set: Average loss: 0.3183, Accuracy: 8865/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.083753 Acc: 0.9688\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.284106 Acc: 0.9375\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.211858 Acc: 0.9062\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.202776 Acc: 0.9375\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.235468 Acc: 0.9375\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.125208 Acc: 1.0000\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.332760 Acc: 0.8750\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.284424 Acc: 0.9062\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.331512 Acc: 0.8750\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.202815 Acc: 0.9062\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.253300 Acc: 0.9062\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.444737 Acc: 0.8438\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.128662 Acc: 0.9688\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.556043 Acc: 0.8750\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.258469 Acc: 0.9062\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.273771 Acc: 0.8750\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.341521 Acc: 0.8750\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.340351 Acc: 0.8750\n",
      "Elapsed 133.07s, 19.01 s/epoch, 0.01 s/batch, ets 247.13s\n",
      "\n",
      "Test set: Average loss: 0.3148, Accuracy: 8888/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.262548 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.529793 Acc: 0.7812\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.301662 Acc: 0.9062\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.249583 Acc: 0.8750\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.306468 Acc: 0.8750\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.418881 Acc: 0.8438\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.226611 Acc: 0.9062\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.293220 Acc: 0.8750\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.255627 Acc: 0.8750\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.222238 Acc: 0.9375\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.337322 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.516131 Acc: 0.8125\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.238938 Acc: 0.8438\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.164269 Acc: 0.9688\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.224884 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.210204 Acc: 0.9688\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.206291 Acc: 0.9375\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.202631 Acc: 0.9062\n",
      "Elapsed 153.31s, 19.16 s/epoch, 0.01 s/batch, ets 229.97s\n",
      "\n",
      "Test set: Average loss: 0.3051, Accuracy: 8899/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.295236 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.275379 Acc: 0.8750\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.345482 Acc: 0.9062\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.192690 Acc: 0.9062\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.129233 Acc: 0.9688\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.178478 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.195175 Acc: 0.9062\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.268638 Acc: 0.8750\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.076569 Acc: 1.0000\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.097564 Acc: 1.0000\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.240914 Acc: 0.9062\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.409577 Acc: 0.7812\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.172627 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.460722 Acc: 0.8125\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.212803 Acc: 0.8438\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.182297 Acc: 0.9375\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.079278 Acc: 0.9688\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.188498 Acc: 0.9375\n",
      "Elapsed 173.31s, 19.26 s/epoch, 0.01 s/batch, ets 211.82s\n",
      "\n",
      "Test set: Average loss: 0.2916, Accuracy: 8962/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.285336 Acc: 0.9062\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.101181 Acc: 0.9375\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.178659 Acc: 0.9688\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.149334 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.163119 Acc: 0.9688\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.184120 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.208369 Acc: 0.9688\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.294363 Acc: 0.8750\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.261915 Acc: 0.8750\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.258041 Acc: 0.8750\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.296018 Acc: 0.9375\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.271146 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.253088 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.367759 Acc: 0.8438\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.288034 Acc: 0.8750\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.366587 Acc: 0.8438\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.157789 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.138899 Acc: 1.0000\n",
      "Elapsed 193.45s, 19.35 s/epoch, 0.01 s/batch, ets 193.45s\n",
      "\n",
      "Test set: Average loss: 0.2953, Accuracy: 8922/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.240128 Acc: 0.9375\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.260694 Acc: 0.9062\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.112575 Acc: 0.9062\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.247600 Acc: 0.9375\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.141053 Acc: 0.9688\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.466589 Acc: 0.9062\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.322680 Acc: 0.9062\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.300739 Acc: 0.9062\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.353351 Acc: 0.8750\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.216186 Acc: 0.8750\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.172344 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.070470 Acc: 1.0000\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.162770 Acc: 0.9375\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.344883 Acc: 0.8125\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.134723 Acc: 0.9688\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.136253 Acc: 0.9375\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.210610 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.460140 Acc: 0.9062\n",
      "Elapsed 213.53s, 19.41 s/epoch, 0.01 s/batch, ets 174.71s\n",
      "\n",
      "Test set: Average loss: 0.2773, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.117039 Acc: 0.9688\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.405141 Acc: 0.8438\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.114641 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.467995 Acc: 0.7812\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.163961 Acc: 0.9062\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.123923 Acc: 0.9688\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.113770 Acc: 0.9688\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.072261 Acc: 0.9688\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.500685 Acc: 0.8750\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.038406 Acc: 1.0000\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.240801 Acc: 0.8750\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.177962 Acc: 0.9688\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.168751 Acc: 0.9062\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.229872 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.142017 Acc: 0.9375\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.102244 Acc: 0.9688\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.133190 Acc: 0.9688\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.439650 Acc: 0.8125\n",
      "Elapsed 233.40s, 19.45 s/epoch, 0.01 s/batch, ets 155.60s\n",
      "\n",
      "Test set: Average loss: 0.2911, Accuracy: 8949/10000 (89%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.250946 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.222210 Acc: 0.9375\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.223966 Acc: 0.9375\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.162542 Acc: 0.9375\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.122647 Acc: 0.9688\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.370426 Acc: 0.9062\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.244467 Acc: 0.9062\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.109434 Acc: 0.9688\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.495403 Acc: 0.8438\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.190263 Acc: 0.9062\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.133812 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.275659 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.180852 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.063391 Acc: 0.9688\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.052055 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.232396 Acc: 0.9062\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.350429 Acc: 0.8125\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.398167 Acc: 0.8750\n",
      "Elapsed 253.41s, 19.49 s/epoch, 0.01 s/batch, ets 136.45s\n",
      "\n",
      "Test set: Average loss: 0.2730, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.313653 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.198647 Acc: 0.9062\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.138315 Acc: 1.0000\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.283136 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.113905 Acc: 0.9688\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.278263 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.137222 Acc: 0.9688\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.247308 Acc: 0.9062\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.167050 Acc: 0.9688\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.135735 Acc: 0.9688\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.542568 Acc: 0.7500\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.173589 Acc: 0.9375\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.307992 Acc: 0.8750\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.017757 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.091481 Acc: 1.0000\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.368108 Acc: 0.9062\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.295009 Acc: 0.8750\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.159369 Acc: 0.9375\n",
      "Elapsed 273.52s, 19.54 s/epoch, 0.01 s/batch, ets 117.22s\n",
      "\n",
      "Test set: Average loss: 0.2889, Accuracy: 8975/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.143330 Acc: 0.9375\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.402707 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.381812 Acc: 0.8750\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.130200 Acc: 0.9688\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.285354 Acc: 0.8438\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.147871 Acc: 0.9688\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.456909 Acc: 0.8750\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.099961 Acc: 0.9688\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.477084 Acc: 0.8750\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.139512 Acc: 0.9375\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.265880 Acc: 0.9062\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.199233 Acc: 0.9375\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.142269 Acc: 0.9688\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.263098 Acc: 0.9062\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.158395 Acc: 0.9688\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.090310 Acc: 1.0000\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.157217 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.337230 Acc: 0.9375\n",
      "Elapsed 293.50s, 19.57 s/epoch, 0.01 s/batch, ets 97.83s\n",
      "\n",
      "Test set: Average loss: 0.2842, Accuracy: 8976/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.354936 Acc: 0.9062\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.210374 Acc: 0.9062\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.114618 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.122450 Acc: 0.9688\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.198104 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.170215 Acc: 0.9375\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.409252 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.139989 Acc: 0.9375\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.331979 Acc: 0.8750\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.240811 Acc: 0.8750\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.281827 Acc: 0.9375\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.141248 Acc: 0.9688\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.258840 Acc: 0.9375\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.348108 Acc: 0.8438\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.406048 Acc: 0.9062\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.194665 Acc: 0.9375\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.146895 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.254398 Acc: 0.8438\n",
      "Elapsed 313.82s, 19.61 s/epoch, 0.01 s/batch, ets 78.46s\n",
      "\n",
      "Test set: Average loss: 0.2834, Accuracy: 8986/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.144789 Acc: 0.9375\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.172100 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.259974 Acc: 0.9062\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.132353 Acc: 0.9688\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.153557 Acc: 0.9375\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.252336 Acc: 0.9062\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.148978 Acc: 0.9688\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.372211 Acc: 0.8125\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.491515 Acc: 0.8125\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.287582 Acc: 0.8750\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.151434 Acc: 0.9062\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.393633 Acc: 0.8438\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.212486 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.253741 Acc: 0.9375\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.163959 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.048955 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.255783 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.045823 Acc: 1.0000\n",
      "Elapsed 333.71s, 19.63 s/epoch, 0.01 s/batch, ets 58.89s\n",
      "\n",
      "Test set: Average loss: 0.2797, Accuracy: 9001/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.219080 Acc: 0.9062\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.151901 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.215543 Acc: 0.8750\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.292070 Acc: 0.8750\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.143964 Acc: 0.9688\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.105202 Acc: 0.9688\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.171949 Acc: 0.9062\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.215841 Acc: 0.9062\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.235984 Acc: 0.9375\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.112536 Acc: 1.0000\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.160270 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.080757 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.084566 Acc: 0.9375\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.428553 Acc: 0.7812\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.268678 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.282309 Acc: 0.8750\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.167563 Acc: 0.9375\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.318812 Acc: 0.8438\n",
      "Elapsed 353.61s, 19.64 s/epoch, 0.01 s/batch, ets 39.29s\n",
      "\n",
      "Test set: Average loss: 0.2790, Accuracy: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.341702 Acc: 0.8750\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.159186 Acc: 0.9375\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.196872 Acc: 0.9375\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.053780 Acc: 1.0000\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.071797 Acc: 0.9688\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.138365 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.067126 Acc: 0.9688\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.277071 Acc: 0.8750\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.071469 Acc: 1.0000\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.126849 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.222766 Acc: 0.9062\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.072059 Acc: 0.9688\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.265982 Acc: 0.9062\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.222917 Acc: 0.8438\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.056985 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.287224 Acc: 0.9062\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.270629 Acc: 0.8750\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.159528 Acc: 0.9688\n",
      "Elapsed 373.59s, 19.66 s/epoch, 0.01 s/batch, ets 19.66s\n",
      "\n",
      "Test set: Average loss: 0.2688, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.357116 Acc: 0.9375\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.125167 Acc: 0.9375\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.227310 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.187851 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.349701 Acc: 0.9375\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.277434 Acc: 0.8750\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.066093 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.213277 Acc: 0.9688\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.066570 Acc: 1.0000\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.203001 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.169938 Acc: 0.9062\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.120332 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.279607 Acc: 0.8750\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.354522 Acc: 0.8750\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.134759 Acc: 0.9375\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.200829 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.249638 Acc: 0.9062\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.148764 Acc: 0.9375\n",
      "Elapsed 393.52s, 19.68 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2788, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Total time: 400.46, Best Loss: 0.269\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curves show that when we use Batch Normalization, the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
    "\n",
    "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
